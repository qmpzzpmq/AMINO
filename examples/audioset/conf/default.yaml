datamodule:
  datasets:
    train: null
    val: null
    test: null
  dataloaders:
    train:
      batch_size: 1
      shuffle: true
      num_workers: 0
    val:
      batch_size: 1
      shuffle: true
      num_workers: 0
    test: null
  collect_fns:
    train:
      pad_choices:
      - pdb
      - unpad
    val:
      pad_choices:
      - pdb
      - unpad
    test:
      pad_choices:
      - pdb
      - unpad
  single_preprocesses:
    train: null
    val: null
    test: null
  after_transform:
    train:
    - select: AMINO.datamodule.preprocess:MelSpectrogram
      conf:
        n_fft: 512
        n_mels: 128
    - select: AMINO.datamodule.preprocess:SpecAug
      conf:
        frequency_mask:
          F: 30
          num_mask: 2
        time_mask:
          T: 40
          num_mask: 2
    val:
    - select: AMINO.datamodule.preprocess:Spectrogram
      conf:
        n_fft: 512
    test:
    - select: AMINO.datamodule.preprocess:FFT
      conf:
        n_fft: 512
module:
  select: AMINO.modules.autoencoder:AMINO_AUTOENCODER
  conf:
    losses: null
    optim:
      select: torch.optim:Adam
      conf:
        lr: 0.01
      contiguous_params: false
    scheduler:
      select: torch.optim.lr_scheduler:StepLR
      conf:
        step_size: 5
        gamma: 0.1
    net:
      select: AMINO.modules.nets.autoencoder:simple_autoencoder
      conf: {}
expbase:
  exp: exp
  tensorboard: tensorboard
  wandb: wandb
  neptune: neptune
  seed: 777
callbacks:
- select: pytorch_lightning.callbacks.progress.tqdm_progress:TQDMProgressBar
  conf:
    refresh_rate: 1
    process_position: 0
- select: pytorch_lightning.callbacks.model_checkpoint:ModelCheckpoint
  conf:
    filename: epoch{epoch}-val_normal_loss{val_normal_loss:.3f}
    monitor: val_normal_loss_epoch
    save_last: true
    save_top_k: 5
    dirpath: checkpoint
- select: pytorch_lightning.callbacks.early_stopping:EarlyStopping
  conf:
    monitor: val_normal_loss_epoch
    mode: min
    min_delta: 1.0e-06
    patience: 30
- select: pytorch_lightning.callbacks:DeviceStatsMonitor
  conf: {}
- select: pytorch_lightning.callbacks.lr_monitor:LearningRateMonitor
  conf:
    logging_interval: epoch
- select: pytorch_lightning.callbacks:ModelSummary
  conf:
    max_depth: 3
loggers:
- select: pytorch_lightning.loggers:TensorBoardLogger
  conf:
    save_dir: ${expbase.tensorboard}/${hydra:job.name}
- select: pytorch_lightning.loggers.wandb:WandbLogger
  conf:
    name: ${hydra:job.name}
    save_dir: null
    project: AMINO
    log_model: false
logging:
  level: DEBUG
trainer:
  accelerator: null
  accumulate_grad_batches: 1
  amp_backend: native
  max_epochs: 100
  min_epochs: 5
  amp_level: null
  auto_lr_find: false
  auto_scale_batch_size: false
  auto_select_gpus: false
  benchmark: false
  fast_dev_run: false
  flush_logs_every_n_steps: 100
  gpus: null
  gradient_clip_val: 50
  gradient_clip_algorithm: norm
  log_every_n_steps: 5
  precision: 32
  replace_sampler_ddp: true
  profiler: null
  strategy: null
variables: null
pipeline_size: null
feature_statistics:
  set: val
checkpoint: null
