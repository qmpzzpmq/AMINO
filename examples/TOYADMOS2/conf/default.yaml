datamodule:
  datasets:
    train: null
    val: null
    test: null
  dataloaders:
    train:
      batch_size: 1
      shuffle: true
      num_workers: 0
    val:
      batch_size: 1
      shuffle: true
      num_workers: 0
    test: null
  single_preprocesses:
    train: null
    val: null
    test: null
  after_transform:
    train:
    - select: AMINO.datamodule.preprocess:MelSpectrogram
      conf:
        n_fft: 512
        n_mels: 128
    - select: AMINO.datamodule.preprocess:SpecAug
      conf:
        frequency_mask:
          F: 30
          num_mask: 2
        time_mask:
          T: 40
          num_mask: 2
        time_streatch:
          floor: 0.9
          ceil: 1.1
    val:
    - select: AMINO.datamodule.preprocess:Spectrogram
      conf:
        n_fft: 512
    test:
    - select: AMINO.datamodule.preprocess:FFT
      conf:
        n_fft: 512
module:
  select: AMINO.modules.autoencoder:AMINO_AUTOENCODER
  conf:
    loss_conf:
      select: torch.nn:MSELoss
      conf:
        reduction: none
    optim_conf:
      select: torch.optim:Adam
      conf:
        lr: 0.001
      contiguous_params: false
    scheduler_conf:
      select: lambdalr
      conf:
        last_epoch: -1
        lr_lambda: 'lambda epoch: 0.95 ** epoch'
    net_conf:
      select: AMINO.modules.nets.autoencoder:simple_autoencoder
      conf: {}
expbase:
  exp: exp
  tensorboard: tensorboard
  wandb: wandb
  neptune: neptune
  seed: 777
callbacks:
  progressbar: true
  progressbar_conf:
    refresh_rate: 1
    process_position: 0
  modelcheckpoint: true
  modelcheckpoint_conf:
    filename: epoch{epoch}-val_normal_loss{val_normal_loss:.3f}
    monitor: val_normal_loss_epoch
    save_last: true
    save_top_k: 5
    dirpath: checkpoint
  earlystopping: true
  earlystopping_conf:
    monitor: val_normal_loss_epoch
    mode: min
    min_delta: 1.0e-06
    patience: 5
  gpu_stats: true
  gpu_stats_conf:
    memory_utilization: true
    gpu_utilization: true
  lr_monitor: true
  lr_monitor_conf:
    logging_interval: epoch
  model_summary: true
  model_summary_conf:
    max_depth: 2
loggers:
  tensorboard: false
  tensorboard_conf:
    save_dir: ${expbase.tensorboard}/${hydra:job.name}
  wandb: false
  wandb_conf:
    name: ${hydra:job.name}
    save_dir: null
    project: AMINO
    log_model: false
  neptune: false
  neptune_conf:
    project_name: ${expbase.neptune}
    experiment_name: ${hydra:job.name}
    api_token: ANONYMOUS
logging:
  level: DEBUG
trainer:
  accelerator: null
  accumulate_grad_batches: 1
  amp_backend: native
  max_epochs: 100
  min_epochs: 5
  amp_level: null
  auto_lr_find: false
  auto_scale_batch_size: false
  auto_select_gpus: false
  benchmark: false
  fast_dev_run: false
  flush_logs_every_n_steps: 100
  gpus: null
  gradient_clip_val: 50
  gradient_clip_algorithm: norm
  log_every_n_steps: 5
  precision: 32
  replace_sampler_ddp: true
  resume_from_checkpoint: ''
  profiler: null
  strategy: null
variables: null
hydra: null
feature_statistics:
  set: val
