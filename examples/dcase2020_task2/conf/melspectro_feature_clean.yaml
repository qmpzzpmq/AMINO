variables:
  datasource: "/haoyu/data/dcase2020_task2/train"
  task_main: "ToyCar" # "ToyTrain" "ToyCar"
  task_aux1: "ToyConveyor"
  batch_size: 256
  dataloaders_num_workers: 5

pipeline_size:
  nfft: 1024
  num_mels: 128
  feature_unfold: 5
  hidden_dims:
    - 128 
    - 32
    - 8

defaults:
  - override hydra/job_logging: AMINO_default
  - override hydra/output: AMINO_default

datamodule:
  datasets:
    train:
      - select: "AMINO.datamodule.datasets:ADMOS_DATASET"
        conf:
          path: ${variables.datasource}/${variables.task_main}/train
          format: "dcase2020_task2"
      - select: "AMINO.datamodule.datasets:ADMOS_DATASET"
        conf:
          path: ${variables.datasource}/${variables.task_aux1}/train
          format: "dcase2020_task2"
          speed_perturb: null
    val:
      - select: "AMINO.datamodule.datasets:ADMOS_DATASET"
        conf:
          path: ${variables.datasource}/${variables.task_main}/test
          format: "dcase2020_task2"
          speed_perturb: null
    test: 
      -  null
  transform:
    batch_after:
      train:
        - select: "AMINO.datamodule.preprocess:MelSpectrogram"
          conf:
            n_fft: ${pipeline_size.nfft}
            n_mels: ${pipeline_size.num_mels}
        - select: "AMINO.datamodule.preprocess:Feature_Unfold"
          conf:
            n_frame: ${pipeline_size.feature_unfold}
        - select: "AMINO.datamodule.preprocess:SpecAug"
          conf:
            frequency_mask:
              F: 30
              num_mask: 2
            time_mask:
              T: 30
              num_mask: 2
            time_streatch:
              floor: 0.9
              ceil: 1.1
      val:
        - select: "AMINO.datamodule.preprocess:MelSpectrogram"
          conf:
            n_fft: ${pipeline_size.nfft}
            n_mels: ${pipeline_size.num_mels}
        - select: "AMINO.datamodule.preprocess:Feature_Unfold"
          conf:
            n_frame: ${pipeline_size.feature_unfold}
      test:
        - select: "AMINO.datamodule.preprocess:MelSpectrogram"
          conf:
            n_fft: ${pipeline_size.nfft}
            n_mels: ${pipeline_size.num_mels}
        - select: "AMINO.datamodule.preprocess:Feature_Unfold"
          conf:
            n_frame: ${pipeline_size.feature_unfold}
  dataloaders:
    train:
      # auto_scale_batch_size 'power' in V100 16G: 1516. might to large
      batch_size: ${variables.batch_size}
      num_workers: ${variables.dataloaders_num_workers}
    val:
      batch_size: ${variables.batch_size}
      num_workers: ${variables.dataloaders_num_workers}
      shuffle: false
  collect_fns:
    train:
      pad_choices: ['pad', 'unpad']
    val:
      pad_choices: ['pad', 'unpad']
    test:
      pad_choices: ['pad', 'unpad']


module:
  # temp use
  select: "AMINO.modules.enc_decs:AMINO_ENC_DECS"
  conf:
    net:
      # temp use
      select: "AMINO.modules.nets.enc_decs:AMINO_ENC_DECS"
      conf:
        encoder:
          select: "AMINO.modules.nets.encoder:SIMPLE_LINEAR_ENCODER"
          conf: 
            feature_dim: '${product: ${pipeline_size.num_mels}, ${pipeline_size.feature_unfold}}'
            hidden_dims: '${pipeline_size.hidden_dims}'
        decoders:
          autoencoder:
            select: "AMINO.modules.nets.decoder:SIMPLE_LINEAR_AUTOENCODER_DECODER"
            conf:
              feature_dim: '${product: ${pipeline_size.num_mels}, ${pipeline_size.feature_unfold}}'
              hidden_dims: '${list_reversed: ${pipeline_size.hidden_dims}}'
    losses:
      nets:
        autoencoder:
          select: torch.nn:MSELoss
          conf:
            reduction: none
      weights:
        autoencoder: 1.0
    optim:
      select: "torch.optim:Adam"
      conf:
        lr: 0.01
    scheduler:
      select: torch.optim.lr_scheduler:StepLR
      conf:
        step_size: 5
        gamma: 0.1

trainer:
  accelerator: null
  accumulate_grad_batches: null
  amp_backend: native
  amp_level: null
  auto_lr_find: false
  auto_scale_batch_size: false
  auto_select_gpus: false
  benchmark: false
  enable_checkpointing: true
  check_val_every_n_epoch: 1
  default_root_dir: null
  detect_anomaly: false
  deterministic: false
  devices: null
  fast_dev_run: false
  gpus: null
  gradient_clip_val: null
  gradient_clip_algorithm: null
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  limit_predict_batches: 1.0
  log_every_n_steps: 1
  enable_progress_bar: true
  profiler: null
  overfit_batches: 0.0
  precision: 32
  max_epochs: null
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  num_nodes: 1
  num_processes: 1
  num_sanity_val_steps: 2
  reload_dataloaders_every_n_epochs: 0
  replace_sampler_ddp: true
  strategy: "ddp"
  sync_batchnorm: false
  terminate_on_nan: null
  tpu_cores: null
  ipus: null
  track_grad_norm: -1
  val_check_interval: 1.0
  weights_save_path: null
  move_metrics_to_cpu: false
  multiple_trainloader_mode: max_size_cycle



callbacks:
- select: pytorch_lightning.callbacks.progress.tqdm_progress:TQDMProgressBar
  conf:
    refresh_rate: 1
    process_position: 0
# - select: pytorch_lightning.callbacks.model_checkpoint:ModelCheckpoint
#   conf:
#     filename: epoch{epoch}-val_loss_total_epoch{val_loss_total_epoch:.3f}
#     monitor: val_loss_total_epoch
#     save_last: true
#     save_top_k: 5
#     dirpath: checkpoint
# - select: pytorch_lightning.callbacks.early_stopping:EarlyStopping
#   conf:
#     monitor: val_loss_total_epoch
#     mode: min
#     min_delta: 1.0e-06
#     patience: 30
- select: pytorch_lightning.callbacks:DeviceStatsMonitor
  conf: {}
- select: pytorch_lightning.callbacks.lr_monitor:LearningRateMonitor
  conf:
    logging_interval: epoch
- select: pytorch_lightning.callbacks:ModelSummary
  conf:
    max_depth: 3

loggers:
- select: pytorch_lightning.loggers:TensorBoardLogger
  conf:
    save_dir: ${expbase.tensorboard}/${hydra:job.name}
- select: pytorch_lightning.loggers.wandb:WandbLogger
  conf:
    name: ${hydra:job.name}
    save_dir: null
    project: AMINO
    log_model: false

expbase:
  exp: exp
  tensorboard: tensorboard
  wandb: wandb
  neptune: neptune
  seed: 777